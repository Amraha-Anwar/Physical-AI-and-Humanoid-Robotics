"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[30],{5680:(e,n,t)=>{t.d(n,{xA:()=>p,yg:()=>u});var o=t(6540);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,o)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach(function(n){a(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function s(e,n){if(null==e)return{};var t,o,a=function(e,n){if(null==e)return{};var t,o,a={},i=Object.keys(e);for(o=0;o<i.length;o++)t=i[o],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)t=i[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var l=o.createContext({}),c=function(e){var n=o.useContext(l),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},p=function(e){var n=c(e.components);return o.createElement(l.Provider,{value:n},e.children)},m="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},g=o.forwardRef(function(e,n){var t=e.components,a=e.mdxType,i=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),m=c(t),g=a,u=m["".concat(l,".").concat(g)]||m[g]||d[g]||i;return t?o.createElement(u,r(r({ref:n},p),{},{components:t})):o.createElement(u,r({ref:n},p))});function u(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var i=t.length,r=new Array(i);r[0]=g;var s={};for(var l in n)hasOwnProperty.call(n,l)&&(s[l]=n[l]);s.originalType=e,s[m]="string"==typeof e?e:a,r[1]=s;for(var c=2;c<i;c++)r[c]=t[c];return o.createElement.apply(null,r)}return o.createElement.apply(null,t)}g.displayName="MDXCreateElement"},9013:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var o=t(8168),a=(t(6540),t(5680));const i={sidebar_position:1,title:"Conversational Robotics"},r="Conversational Robotics: Voice-to-Action",s={unversionedId:"module4/conversational-robotics",id:"module4/conversational-robotics",title:"Conversational Robotics",description:"Learning Objectives",source:"@site/docs/module4/01-conversational-robotics.mdx",sourceDirName:"module4",slug:"/module4/conversational-robotics",permalink:"/docs/module4/conversational-robotics",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module4/01-conversational-robotics.mdx",tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1,title:"Conversational Robotics"},sidebar:"tutorialSidebar",previous:{title:"Nav2 for Humanoids (Conceptual Balance)",permalink:"/docs/module3/nav2-humanoids"},next:{title:"Cognitive Planning with LLMs",permalink:"/docs/module4/cognitive-planning"}},l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Conversational Robotics",id:"introduction-to-conversational-robotics",level:2},{value:"1. Speech Recognition (Speech-to-Text)",id:"1-speech-recognition-speech-to-text",level:2},{value:"Using OpenAI Whisper (Conceptual)",id:"using-openai-whisper-conceptual",level:3},{value:"2. Linking Voice Commands to ROS 2 Topics",id:"2-linking-voice-commands-to-ros-2-topics",level:2},{value:"Integrating with <code>setup.py</code>",id:"integrating-with-setuppy",level:3},{value:"Conclusion",id:"conclusion",level:2}],p={toc:c},m="wrapper";function d({components:e,...n}){return(0,a.yg)(m,(0,o.A)({},p,n,{components:e,mdxType:"MDXLayout"}),(0,a.yg)("h1",{id:"conversational-robotics-voice-to-action"},"Conversational Robotics: Voice-to-Action"),(0,a.yg)("h2",{id:"learning-objectives"},"Learning Objectives"),(0,a.yg)("p",null,"Upon completing this chapter, you will be able to:"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Understand the concept of Voice-to-Action in robotics and its importance for natural human-robot interaction."),(0,a.yg)("li",{parentName:"ul"},"Identify key components of a speech recognition pipeline for robotic control."),(0,a.yg)("li",{parentName:"ul"},"Learn how to integrate speech recognition technology (e.g., OpenAI Whisper or similar) to process natural language."),(0,a.yg)("li",{parentName:"ul"},"Translate transcribed voice commands into actionable ROS 2 messages for robot execution."),(0,a.yg)("li",{parentName:"ul"},"Develop a basic ROS 2 node to listen for voice commands and publish to a downstream topic.")),(0,a.yg)("h2",{id:"introduction-to-conversational-robotics"},"Introduction to Conversational Robotics"),(0,a.yg)("p",null,"Conversational robotics aims to enable robots to understand and respond to human speech, making interaction more intuitive and accessible. The goal is to move beyond rigid command structures to natural language interfaces, allowing users to direct robots with spoken instructions. This ",(0,a.yg)("strong",{parentName:"p"},"Voice-to-Action")," paradigm involves several stages: speech capture, speech-to-text conversion, natural language understanding (NLU), and finally, mapping NLU output to robot actions."),(0,a.yg)("p",null,"For a humanoid robot, this capability opens up possibilities for:"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},'Direct task assignment (e.g., "Walk to the door," "Pick up the cup").'),(0,a.yg)("li",{parentName:"ul"},'Querying robot status (e.g., "What are you doing?").'),(0,a.yg)("li",{parentName:"ul"},'Emergency commands (e.g., "Stop!").')),(0,a.yg)("h2",{id:"1-speech-recognition-speech-to-text"},"1. Speech Recognition (Speech-to-Text)"),(0,a.yg)("p",null,"The first step in Voice-to-Action is converting spoken words into written text. This is handled by ",(0,a.yg)("strong",{parentName:"p"},"Speech-to-Text (STT)")," engines. While many options exist (Google Cloud Speech-to-Text, Amazon Transcribe, Mozilla DeepSpeech), ",(0,a.yg)("strong",{parentName:"p"},"OpenAI Whisper")," has emerged as a highly capable, multilingual, and robust open-source STT model."),(0,a.yg)("h3",{id:"using-openai-whisper-conceptual"},"Using OpenAI Whisper (Conceptual)"),(0,a.yg)("p",null,"Whisper can be run locally or via API. For an edge device like the Jetson Orin Nano, running a smaller Whisper model locally is feasible for real-time applications."),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Conceptual Workflow:")),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Audio Capture"),": The robot's microphone captures audio input from the user."),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Preprocessing"),": Audio is often preprocessed (e.g., noise reduction, normalization) before being fed to the STT model."),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Transcription"),": The STT model converts the audio into a text transcript.")),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Pseudo-code Example (Python with a conceptual Whisper library):")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n# from your_audio_pkg.msg import AudioData # Assuming an AudioData message type\nimport numpy as np\n# import whisper # conceptual import\n\nclass VoiceCommandTranscriber(Node):\n    def __init__(self):\n        super().__init__('voice_command_transcriber')\n        # Publisher for transcribed text\n        self.transcribed_text_pub = self.create_publisher(String, 'voice_commands/text', 10)\n        # Subscriber for raw audio data (conceptual)\n        # self.audio_sub = self.create_subscription(AudioData, 'audio_in', self.audio_callback, 10)\n        \n        # Simulate audio input for demonstration\n        self.timer = self.create_timer(5.0, self.simulate_audio_input)\n        self.get_logger().info('Voice command transcriber node started.')\n\n    def audio_callback(self, msg):\n        # Process raw audio data (e.g., convert to format Whisper expects)\n        # audio_np = np.frombuffer(msg.data, dtype=np.int16) # Example conversion\n\n        # Perform speech-to-text (conceptual)\n        # model = whisper.load_model(\"base\") # Load a Whisper model\n        # result = model.transcribe(audio_np)\n        # transcribed_text = result[\"text\"]\n\n        # For simulation, we'll use a hardcoded command\n        transcribed_text = \"robot walk forward\" # This would come from Whisper\n\n        if transcribed_text:\n            self.get_logger().info(f'Transcribed: \"{transcribed_text}\"')\n            msg_text = String()\n            msg_text.data = transcribed_text.lower() # Normalize to lowercase\n            self.transcribed_text_pub.publish(msg_text)\n\n    def simulate_audio_input(self):\n        # This function simulates receiving audio input\n        # In a real system, this would be an actual audio callback\n        self.get_logger().info('Simulating audio input...')\n        self.audio_callback(None) # Call the callback with dummy data\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceCommandTranscriber()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n")),(0,a.yg)("p",null,(0,a.yg)("em",{parentName:"p"},"Pseudo-code: ",(0,a.yg)("inlineCode",{parentName:"em"},"voice_command_transcriber.py"))),(0,a.yg)("h2",{id:"2-linking-voice-commands-to-ros-2-topics"},"2. Linking Voice Commands to ROS 2 Topics"),(0,a.yg)("p",null,"Once we have a text transcript, the next step is to translate this natural language into a structured, actionable command that the robot's control system can understand. This often involves a simple form of ",(0,a.yg)("strong",{parentName:"p"},"Natural Language Understanding (NLU)"),". For basic commands, rule-based parsing can be sufficient. For more complex interactions, an LLM might be employed (covered in the next chapter)."),(0,a.yg)("p",null,'We\'ll create a simple "command parser" node that subscribes to the transcribed text and publishes a specific command message to a downstream ROS 2 topic.'),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Conceptual Workflow:")),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Subscribe to Transcribed Text"),": The command parser node listens to the ",(0,a.yg)("inlineCode",{parentName:"li"},"voice_commands/text")," topic."),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Parse Command"),": It analyzes the text to identify keywords or patterns that correspond to known robot actions."),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Publish Action Message"),": Based on the parsed command, it publishes a specific message to a robot action topic (e.g., ",(0,a.yg)("inlineCode",{parentName:"li"},"/cmd_vel")," for movement, or a custom action goal).")),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Pseudo-code Example (Python):")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist # For velocity commands\n\nclass CommandParser(Node):\n    def __init__(self):\n        super().__init__(\'command_parser\')\n        # Subscriber for transcribed text commands\n        self.text_sub = self.create_subscription(String, \'voice_commands/text\', self.text_command_callback, 10)\n        # Publisher for robot movement commands (e.g., cmd_vel)\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n        \n        self.get_logger().info(\'Command parser node started. Waiting for voice commands...\')\n\n    def text_command_callback(self, msg):\n        command_text = msg.data\n        self.get_logger().info(f\'Received command: "{command_text}"\')\n\n        twist_msg = Twist()\n        \n        # Simple rule-based parsing\n        if "walk forward" in command_text:\n            self.get_logger().info("Executing: Walk Forward")\n            twist_msg.linear.x = 0.2 # Move forward at 0.2 m/s\n        elif "turn left" in command_text:\n            self.get_logger().info("Executing: Turn Left")\n            twist_msg.angular.z = 0.5 # Turn left at 0.5 rad/s\n        elif "stop" in command_text:\n            self.get_logger().info("Executing: Stop")\n            twist_msg.linear.x = 0.0\n            twist_msg.angular.z = 0.0\n        else:\n            self.get_logger().warn(f"Unknown command: \'{command_text}\'")\n            return # Don\'t publish if command is not recognized\n\n        self.cmd_vel_pub.publish(twist_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CommandParser()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n')),(0,a.yg)("p",null,(0,a.yg)("em",{parentName:"p"},"Pseudo-code: ",(0,a.yg)("inlineCode",{parentName:"em"},"command_parser.py"))),(0,a.yg)("h3",{id:"integrating-with-setuppy"},"Integrating with ",(0,a.yg)("inlineCode",{parentName:"h3"},"setup.py")),(0,a.yg)("p",null,"Remember to add these new nodes as entry points in your ",(0,a.yg)("inlineCode",{parentName:"p"},"setup.py")," within your ROS 2 Python package (e.g., ",(0,a.yg)("inlineCode",{parentName:"p"},"my_py_pkg")," or a new ",(0,a.yg)("inlineCode",{parentName:"p"},"humanoid_vla")," package):"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},"# ... inside setup() function ...\n    entry_points={\n        'console_scripts': [\n            'voice_transcriber = my_py_pkg.voice_command_transcriber:main',\n            'command_parser = my_py_pkg.command_parser:main',\n        ],\n    },\n")),(0,a.yg)("h2",{id:"conclusion"},"Conclusion"),(0,a.yg)("p",null,"Conversational robotics empowers humanoids with a more natural interaction paradigm. By integrating speech recognition to convert voice to text, and then parsing these text commands into ROS 2 messages, we can enable intuitive control over our robot. This forms the foundation for more advanced cognitive abilities where Large Language Models can interpret complex instructions."),(0,a.yg)("hr",null),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Next Steps"),": Building upon speech recognition, the next chapter will delve into ",(0,a.yg)("strong",{parentName:"p"},"Cognitive Planning")," using Large Language Models (LLMs), allowing the robot to understand more abstract goals, reason about its environment, and generate complex action sequences."))}d.isMDXComponent=!0}}]);