"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[819],{5680:(e,n,t)=>{t.d(n,{xA:()=>p,yg:()=>g});var a=t(6540);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,a)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach(function(n){i(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function l(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var s=a.createContext({}),c=function(e){var n=a.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},p=function(e){var n=c(e.components);return a.createElement(s.Provider,{value:n},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},d=a.forwardRef(function(e,n){var t=e.components,i=e.mdxType,o=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),u=c(t),d=i,g=u["".concat(s,".").concat(d)]||u[d]||m[d]||o;return t?a.createElement(g,r(r({ref:n},p),{},{components:t})):a.createElement(g,r({ref:n},p))});function g(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var o=t.length,r=new Array(o);r[0]=d;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[u]="string"==typeof e?e:i,r[1]=l;for(var c=2;c<o;c++)r[c]=t[c];return a.createElement.apply(null,r)}return a.createElement.apply(null,t)}d.displayName="MDXCreateElement"},8205:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>l,toc:()=>c});var a=t(8168),i=(t(6540),t(5680));const o={sidebar_position:3,title:"Capstone Synthesis & Sim-to-Real"},r="Capstone Synthesis & Sim-to-Real",l={unversionedId:"module4/capstone-synthesis",id:"module4/capstone-synthesis",title:"Capstone Synthesis & Sim-to-Real",description:"Learning Objectives",source:"@site/docs/module4/03-capstone-synthesis.mdx",sourceDirName:"module4",slug:"/module4/capstone-synthesis",permalink:"/docs/module4/capstone-synthesis",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module4/03-capstone-synthesis.mdx",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3,title:"Capstone Synthesis & Sim-to-Real"},sidebar:"tutorialSidebar",previous:{title:"Cognitive Planning with LLMs",permalink:"/docs/module4/cognitive-planning"}},s={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Capstone Synthesis",id:"introduction-to-capstone-synthesis",level:2},{value:"1. End-to-End Architecture Overview",id:"1-end-to-end-architecture-overview",level:2},{value:"2. Orchestrating the System (Conceptual Launch File)",id:"2-orchestrating-the-system-conceptual-launch-file",level:2},{value:"3. Sim-to-Real Model/Config Transfer",id:"3-sim-to-real-modelconfig-transfer",level:2},{value:"3.1 Boilerplate for Model and Config Transfer",id:"31-boilerplate-for-model-and-config-transfer",level:3},{value:"4. Final Validation",id:"4-final-validation",level:2},{value:"Checklist for Sim-to-Real Validation:",id:"checklist-for-sim-to-real-validation",level:3},{value:"Conclusion",id:"conclusion",level:2}],p={toc:c},u="wrapper";function m({components:e,...n}){return(0,i.yg)(u,(0,a.A)({},p,n,{components:e,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"capstone-synthesis--sim-to-real"},"Capstone Synthesis & Sim-to-Real"),(0,i.yg)("h2",{id:"learning-objectives"},"Learning Objectives"),(0,i.yg)("p",null,"Upon completing this chapter, you will be able to:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},"Integrate the Conversational Robotics (VLA) node, Nav2 for humanoids, and (conceptual) manipulation capabilities into a cohesive autonomous system."),(0,i.yg)("li",{parentName:"ul"},"Understand the end-to-end architecture of a humanoid robot controlled by natural language commands."),(0,i.yg)("li",{parentName:"ul"},"Implement procedures for transferring models and configurations from your workstation (simulation) to the Jetson Edge Kit (real robot)."),(0,i.yg)("li",{parentName:"ul"},"Grasp the final steps of validating your integrated system in both simulation and the real world.")),(0,i.yg)("h2",{id:"introduction-to-capstone-synthesis"},"Introduction to Capstone Synthesis"),(0,i.yg)("p",null,"Throughout this book, we've built foundational knowledge and implemented individual modules for our Physical AI humanoid robot:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Core Robotics (ROS 2, URDF)"),": Understanding the robot's nervous system."),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"Digital Twin (Gazebo)"),": Simulating physics, sensors, and environments."),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"AI-Robot Brain (Isaac Sim, VSLAM, Nav2)"),": Providing perception and navigation intelligence."),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("strong",{parentName:"li"},"VLA Integration (Conversational Robotics, Cognitive Planning)"),": Enabling natural language understanding and high-level task planning.")),(0,i.yg)("p",null,"This capstone chapter brings all these pieces together. The goal is to orchestrate these capabilities to create an autonomous humanoid robot that can understand natural language goals, plan its actions, navigate its environment, and interact with objects, demonstrating the full power of a Physical AI system."),(0,i.yg)("h2",{id:"1-end-to-end-architecture-overview"},"1. End-to-End Architecture Overview"),(0,i.yg)("p",null,"The final autonomous humanoid system can be conceptualized as a hierarchical control architecture, with the LLM providing high-level cognitive function, breaking down abstract goals into a sequence of low-level robot actions."),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-mermaid"},"graph TD\n    A[Human Voice Command] --\x3e B(Microphone/Audio Input)\n    B --\x3e C(Speech-to-Text - Whisper Node)\n    C --\x3e D(Transcribed Text - ROS Topic: /voice_commands/text)\n    D --\x3e E(Cognitive Planner - LLM Node)\n    E --\x3e F(Action Plan - ROS Service/Action: /robot_task_plan)\n\n    subgraph Robot Action Execution (ROS 2 Graph)\n        F --\x3e G(Action Executor/Sequencer Node)\n        G --\x3e H1(Navigation Controller - Nav2)\n        G --\x3e H2(Manipulation Controller)\n        G --\x3e H3(Perception Query - VSLAM)\n    end\n\n    subgraph Navigation Stack (Nav2)\n        H1 --\x3e I1(Global Planner)\n        H1 --\x3e I2(Humanoid Local Planner/Footstep)\n        H1 --\x3e I3(Whole-Body Controller/Balance)\n    end\n\n    subgraph Perception (Isaac ROS)\n        H3 --\x3e J1(Camera/IMU Drivers)\n        H3 --\x3e J2(VSLAM Node - Isaac ROS)\n        J2 --\x3e K(Pose / Map Data - ROS Topics: /tf, /map)\n    end\n\n    subgraph Simulation/Hardware\n        I3 --\x3e L(Actuator Commands)\n        K --\x3e L\n        L --\x3e M[Humanoid Robot (Sim/Real)]\n        M --\x3e J1\n    end\n\n    style A fill:#cef,stroke:#333,stroke-width:2px\n    style B fill:#f9f,stroke:#333,stroke-width:2px\n    style C fill:#bbf,stroke:#333,stroke-width:2px\n    style D fill:#dbf,stroke:#333,stroke-width:2px\n    style E fill:#fcf,stroke:#333,stroke-width:2px\n    style F fill:#cfc,stroke:#333,stroke-width:2px\n    style G fill:#ffc,stroke:#333,stroke-width:2px\n    style H1 fill:#cff,stroke:#333,stroke-width:2px\n    style H2 fill:#ffc,stroke:#333,stroke-width:2px\n    style H3 fill:#ffc,stroke:#333,stroke-width:2px\n    style I1 fill:#aaf,stroke:#333,stroke-width:2px\n    style I2 fill:#afa,stroke:#333,stroke-width:2px\n    style I3 fill:#faa,stroke:#333,stroke-width:2px\n    style J1 fill:#eee,stroke:#333,stroke-width:2px\n    style J2 fill:#dde,stroke:#333,stroke-width:2px\n    style K fill:#eff,stroke:#333,stroke-width:2px\n    style L fill:#fcd,stroke:#333,stroke-width:2px\n    style M fill:#f99,stroke:#333,stroke-width:2px\n")),(0,i.yg)("p",null,(0,i.yg)("em",{parentName:"p"},"Figure 1: Integrated End-to-End Humanoid Robot Architecture.")),(0,i.yg)("h2",{id:"2-orchestrating-the-system-conceptual-launch-file"},"2. Orchestrating the System (Conceptual Launch File)"),(0,i.yg)("p",null,"To launch this entire system, you would typically use a complex ROS 2 launch file that starts all necessary nodes and configures their parameters."),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"import os\nfrom ament_index_python.packages import get_package_share_directory\nfrom launch import LaunchDescription\nfrom launch.actions import IncludeLaunchDescription, DeclareLaunchArgument\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom launch.substitutions import LaunchConfiguration\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    pkg_humanoid_description = get_package_share_directory('humanoid_description')\n    pkg_humanoid_vla = get_package_share_directory('humanoid_vla') # Assuming a package for VLA nodes\n    pkg_humanoid_navigation = get_package_share_directory('humanoid_navigation') # Assuming a package for Nav2 config\n\n    # Declare arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='true')\n    urdf_file = os.path.join(pkg_humanoid_description, 'urdf', 'humanoid_full.urdf') # Full robot URDF\n    world_file = os.path.join(pkg_humanoid_description, 'worlds', 'humanoid_arena.world') # Complex world\n\n    return LaunchDescription([\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='true',\n            description='Use simulation (Gazebo) clock if true'),\n\n        # 1. Launch Gazebo and spawn robot (including simulated sensors)\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource(\n                os.path.join(get_package_share_directory('ros_ign_gazebo'), 'launch', 'ign_gazebo.launch.py')\n            ),\n            launch_arguments={'ign_args': ['-r ', world_file]}.items()\n        ),\n        Node(\n            package='ros_ign_gazebo',\n            executable='create',\n            output='screen',\n            arguments=['-string', open(urdf_file).read(),\n                       '-name', 'humanoid_robot',\n                       '-x', '0', '-y', '0', '-z', '0.5']\n        ),\n        Node(\n            package='robot_state_publisher',\n            executable='robot_state_publisher',\n            name='robot_state_publisher',\n            output='screen',\n            parameters=[{'robot_description': open(urdf_file).read(), 'use_sim_time': use_sim_time}]\n        ),\n        Node(\n            package='joint_state_publisher_gui', # For simulation control\n            executable='joint_state_publisher_gui',\n            name='joint_state_publisher_gui',\n            output='screen'\n        ),\n\n        # 2. VLA Stack (Conversational Robotics & Cognitive Planning)\n        Node(\n            package='humanoid_vla',\n            executable='voice_command_transcriber',\n            name='voice_command_transcriber',\n            output='screen',\n            parameters=[{'use_sim_time': use_sim_time}]\n        ),\n        Node(\n            package='humanoid_vla',\n            executable='cognitive_planner',\n            name='cognitive_planner',\n            output='screen',\n            parameters=[{'use_sim_time': use_sim_time}]\n        ),\n\n        # 3. Isaac ROS VSLAM (Perception)\n        # Assuming you'd launch Isaac ROS VSLAM through its own launch file or directly via Docker\n        # For simplicity, we'll just include a placeholder for its integration.\n        # This would typically be a set of nodes consuming camera/IMU data and publishing /tf and /map\n        # E.g., IncludeLaunchDescription(PythonLaunchDescriptionSource(os.path.join(\n        #     get_package_share_directory('isaac_ros_visual_slam'), 'launch', 'isaac_ros_visual_slam.launch.py'\n        # )), launch_arguments={'use_sim_time': use_sim_time}.items()),\n\n        # 4. Nav2 Stack for Humanoids\n        # This would be a specialized Nav2 configuration for bipedal robots\n        IncludeLaunchDescription(\n            PythonLaunchDescriptionSource(\n                os.path.join(pkg_humanoid_navigation, 'launch', 'humanoid_nav2_bringup.launch.py')\n            ),\n            launch_arguments={'use_sim_time': use_sim_time}.items()\n        ),\n\n        # 5. Manipulation Controller (conceptual)\n        # Node(\n        #     package='humanoid_manipulation',\n        #     executable='manipulation_controller',\n        #     name='manipulation_controller',\n        #     output='screen',\n        #     parameters=[{'use_sim_time': use_sim_time}]\n        # ),\n    ])\n")),(0,i.yg)("p",null,(0,i.yg)("em",{parentName:"p"},"Pseudo-code: ",(0,i.yg)("inlineCode",{parentName:"em"},"humanoid_full_system.launch.py"))),(0,i.yg)("p",null,"This launch file starts Gazebo, spawns the robot, and initializes all the core ROS 2 nodes for VLA, perception, and navigation."),(0,i.yg)("h2",{id:"3-sim-to-real-modelconfig-transfer"},"3. Sim-to-Real Model/Config Transfer"),(0,i.yg)("p",null,"A crucial step in deploying Physical AI robots is transferring the developed models, configurations, and trained neural networks from the powerful workstation (where simulation and training often occur) to the resource-constrained Jetson Edge Kit."),(0,i.yg)("h3",{id:"31-boilerplate-for-model-and-config-transfer"},"3.1 Boilerplate for Model and Config Transfer"),(0,i.yg)("p",null,"This typically involves a combination of version control, build systems, and file transfer."),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Version Control (Git)"),": All your ROS 2 packages, URDFs, world files, launch files, and configuration files (",(0,i.yg)("inlineCode",{parentName:"li"},".yaml"),") should be in a Git repository."),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},(0,i.yg)("inlineCode",{parentName:"strong"},"colcon")," Build"),": Ensure all your ROS 2 packages build correctly on the target architecture (ARM64 for Jetson). Cross-compilation or building directly on the Jetson are common strategies."),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Docker Containers (Recommended for Isaac ROS/AI Models)"),": For AI components (like Isaac ROS VSLAM or custom neural networks), Docker is the preferred method for deployment.",(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"Build your Docker images on the workstation (or pull pre-built NVIDIA images)."),(0,i.yg)("li",{parentName:"ul"},"Push them to a container registry (e.g., NVIDIA NGC, Docker Hub, private registry)."),(0,i.yg)("li",{parentName:"ul"},"Pull and run the images on the Jetson.")))),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Example ",(0,i.yg)("inlineCode",{parentName:"strong"},"Dockerfile")," for a custom Python ROS 2 node with LLM integration:")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-dockerfile"},'# Base image for ROS 2 on Jetson (e.g., from NVIDIA L4T or ROS official)\nFROM nvcr.io/nvidia/l4t-ros-humble:jp5.1.2-ros-base\n\n# Set environment variables for ROS 2\nENV ROS_DISTRO humble\nENV ROS_WS /opt/ros_ws\n\n# Install Python dependencies\nRUN apt update && apt install -y \\\n    python3-pip \\\n    # ... other dependencies like libsndfile1 for Whisper ...\n && rm -rf /var/lib/apt/lists/*\n\n# Install OpenAI Python client (or Whisper-related packages)\nRUN pip install openai # For LLM API calls\n\n# Copy your ROS 2 workspace\nCOPY ./humanoid_vla $ROS_WS/src/humanoid_vla\nCOPY ./humanoid_description $ROS_WS/src/humanoid_description\n# Add other custom packages\n\n# Build the ROS 2 workspace\nWORKDIR $ROS_WS\nRUN /bin/bash -c "source /opt/ros/$ROS_DISTRO/setup.bash && colcon build"\n\n# Source the workspace setup file (for subsequent commands and entrypoint)\nRUN echo "source $ROS_WS/install/setup.bash" >> ~/.bashrc\n# CMD ["/bin/bash", "-c", "source /opt/ros_ws/install/setup.bash && exec bash"]\n')),(0,i.yg)("p",null,(0,i.yg)("em",{parentName:"p"},"File: ",(0,i.yg)("inlineCode",{parentName:"em"},"Dockerfile.jetson"))),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Deployment Procedure:")),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Build/Update Docker Image"),": On your workstation, build the Docker image (if not using pre-built ones):",(0,i.yg)("pre",{parentName:"li"},(0,i.yg)("code",{parentName:"pre",className:"language-bash"},"docker build -f Dockerfile.jetson -t my_humanoid_robot:latest .\ndocker push my_registry/my_humanoid_robot:latest\n"))),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Transfer Configuration Files"),":",(0,i.yg)("ul",{parentName:"li"},(0,i.yg)("li",{parentName:"ul"},"ROS 2 parameter files (",(0,i.yg)("inlineCode",{parentName:"li"},".yaml"),"): These define node behaviors. They can be part of your ROS 2 package and transferred via Git, or managed separately."),(0,i.yg)("li",{parentName:"ul"},"LLM API Keys: ",(0,i.yg)("strong",{parentName:"li"},"NEVER hardcode API keys.")," Use environment variables or a secure secret management system on the Jetson."),(0,i.yg)("li",{parentName:"ul"},"Trained AI models (e.g., a fine-tuned Whisper model): If running locally, these might be copied directly or baked into a Docker image."))),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Execute on Jetson"),": SSH into your Jetson Orin Nano, pull the latest Docker image, and run it. Your ROS 2 launch files will then start the nodes inside the container.")),(0,i.yg)("h2",{id:"4-final-validation"},"4. Final Validation"),(0,i.yg)("p",null,"After deploying to the Jetson Edge Kit, comprehensive validation is necessary."),(0,i.yg)("h3",{id:"checklist-for-sim-to-real-validation"},"Checklist for Sim-to-Real Validation:"),(0,i.yg)("ul",{className:"contains-task-list"},(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ",(0,i.yg)("strong",{parentName:"li"},"Power-up and Boot"),": Robot and Jetson power on correctly."),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ",(0,i.yg)("strong",{parentName:"li"},"ROS 2 Communication"),": All ROS 2 nodes are launching and communicating (check ",(0,i.yg)("inlineCode",{parentName:"li"},"ros2 topic list"),", ",(0,i.yg)("inlineCode",{parentName:"li"},"ros2 node list"),", ",(0,i.yg)("inlineCode",{parentName:"li"},"ros2 graph"),")."),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ",(0,i.yg)("strong",{parentName:"li"},"Sensor Data"),":",(0,i.yg)("ul",{parentName:"li",className:"contains-task-list"},(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","RealSense camera is publishing RGB, Depth, and IMU data to correct ROS 2 topics."),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Other real sensors (e.g., motor encoders) are publishing data."))),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ",(0,i.yg)("strong",{parentName:"li"},"Perception (VSLAM)"),":",(0,i.yg)("ul",{parentName:"li",className:"contains-task-list"},(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Isaac ROS VSLAM node is running and publishing accurate pose (",(0,i.yg)("inlineCode",{parentName:"li"},"/tf"),") and map data."),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","RViz2 visualization on the workstation (via network) shows the robot correctly localized."))),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ",(0,i.yg)("strong",{parentName:"li"},"VLA (Conversational Robotics)"),":",(0,i.yg)("ul",{parentName:"li",className:"contains-task-list"},(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Microphone input is being transcribed correctly (test with ",(0,i.yg)("inlineCode",{parentName:"li"},"ros2 run humanoid_vla voice_command_transcriber"),")."),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Cognitive planner translates natural language to action plans (",(0,i.yg)("inlineCode",{parentName:"li"},"ros2 topic echo /robot_task_plan"),")."))),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ",(0,i.yg)("strong",{parentName:"li"},"Navigation"),":",(0,i.yg)("ul",{parentName:"li",className:"contains-task-list"},(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Given a navigation goal (e.g., from LLM), the robot attempts to move."),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Humanoid footstep planner is generating feasible steps."),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Robot maintains balance during locomotion."))),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ",(0,i.yg)("strong",{parentName:"li"},"Manipulation (if implemented)"),":",(0,i.yg)("ul",{parentName:"li",className:"contains-task-list"},(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Robot can grasp and manipulate objects as commanded."))),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ",(0,i.yg)("strong",{parentName:"li"},"Safety"),":",(0,i.yg)("ul",{parentName:"li",className:"contains-task-list"},(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Emergency stop mechanisms are functional."),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Robot operates within safe limits."))),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ",(0,i.yg)("strong",{parentName:"li"},"Performance"),":",(0,i.yg)("ul",{parentName:"li",className:"contains-task-list"},(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Check CPU/GPU utilization on Jetson (e.g., ",(0,i.yg)("inlineCode",{parentName:"li"},"jtop"),")."),(0,i.yg)("li",{parentName:"ul",className:"task-list-item"},(0,i.yg)("input",{parentName:"li",type:"checkbox",checked:!1,disabled:!0})," ","Verify ROS 2 topic rates are within expected bounds.")))),(0,i.yg)("h2",{id:"conclusion"},"Conclusion"),(0,i.yg)("p",null,"The journey of building a Physical AI humanoid robot culminates in this capstone synthesis. By integrating modular ROS 2 components, leveraging advanced perception with Isaac ROS, and enabling high-level cognitive planning with LLMs, you have created an intelligent autonomous system. The Sim-to-Real transfer process ensures that your hard work in simulation can be effectively deployed and validated on the physical robot, bridging the gap between the virtual and real worlds. This integrated approach is the future of advanced robotics."),(0,i.yg)("hr",null),(0,i.yg)("p",null,(0,i.yg)("strong",{parentName:"p"},"Final Word"),": Robotics is an iterative process. Continue to refine, test, and expand your robot's capabilities. The tools and concepts learned here provide a strong foundation for a career at the forefront of Physical AI."))}m.isMDXComponent=!0}}]);