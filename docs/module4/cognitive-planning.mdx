---
sidebar_position: 2
title: "Cognitive Planning with LLMs"
---

# Cognitive Planning with LLMs

Now that our robot has "Ears" (Whisper), it needs a "Brain" to understand intent. We use a **Large Language Model (LLM)** to translate natural language into ROS 2 actions.

## 2.1 The System Prompt

The key to making an LLM control a robot is the **System Prompt**. We must define the robot's capabilities (API) so the LLM knows what tools it has.

Example Prompt:
> "You are a robot assistant. You can execute the following functions: `navigate_to(location)`, `say(text)`. Convert the user's request into a JSON sequence of these functions. Do not output explanations."

## 2.2 The Planner Node

This node subscribes to `/human_command` (from Chapter 4.1) and calls the OpenAI API to generate a plan.

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from openai import OpenAI
import json

class CognitivePlanner(Node):
    def __init__(self):
        super().__init__('cognitive_planner')
        self.subscription = self.create_subscription(
            String, '/human_command', self.command_callback, 10)
        self.client = OpenAI()
        
        self.system_prompt = """
        You are a humanoid robot. Available actions:
        - navigate_to(location_name): Moves to a saved location (kitchen, lobby).
        - wave_hand(): Waves the right hand.
        
        Output JSON format: {"actions": [{"name": "navigate_to", "args": ["kitchen"]}]}
        """

    def command_callback(self, msg):
        user_text = msg.data
        self.get_logger().info(f"Planning for: {user_text}")
        
        response = self.client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": user_text}
            ],
            response_format={"type": "json_object"}
        )
        
        plan_json = response.choices[0].message.content
        self.execute_plan(json.loads(plan_json))

    def execute_plan(self, plan):
        for action in plan['actions']:
            self.get_logger().info(f"EXECUTING: {action['name']} with {action.get('args', [])}")
            # Here we would call the actual ROS 2 Action Client
            # e.g., send_goal(NavigateToPose, ...)

def main():
    rclpy.init()
    node = CognitivePlanner()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()
```

## 2.3 Vision-Language-Action (VLA)

Strictly speaking, an LLM processes text. A **VLA** model (like GPT-4o with Vision) can also process images.
*   **Scenario**: The user says "Pick up the red apple."
*   **Pipeline**:
    1.  Capture image from RealSense.
    2.  Send Image + Prompt to VLA.
    3.  VLA returns bounding box of "red apple" [x, y, w, h].
    4.  Robot plans grasp to that 3D coordinate.

:::info Capstone Linkage
In the Capstone, this node is the "Conductor." It orchestrates the VSLAM (Module 3) and the Low-level Control (Module 1) based on high-level intent.
:::
