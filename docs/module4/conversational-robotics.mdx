---
sidebar_position: 1
title: "Conversational Robotics (Voice-to-Action)"
---

# Conversational Robotics: Voice-to-Action

The final layer of our Autonomous Humanoid is the ability to understand natural language. We start by giving the robot "ears" using **Automatic Speech Recognition (ASR)**.

## 1.1 The Audio Pipeline

1.  **Microphone**: Captures raw audio waveforms.
2.  **ASR Model**: Transcribes waveform -> Text ("Go to the kitchen").
3.  **LLM**: Interprets Text -> Intent (See Chapter 4.2).

## 1.2 Using OpenAI Whisper

We will use **OpenAI Whisper**, a robust, multi-lingual ASR model. We can run the small version locally on the Jetson or use the API. For this book, we use the API for simplicity and accuracy.

### Prerequisites
*   A USB Microphone connected to the Jetson/PC.
*   `pip install openai sounddevice numpy scipy`

## 1.3 The Voice Node

We create a ROS 2 node `voice_listener.py` that listens for audio triggers and publishes the transcribed text.

```python
#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
import sounddevice as sd
import scipy.io.wavfile as wav
from openai import OpenAI
import os

class VoiceListener(Node):
    def __init__(self):
        super().__init__('voice_listener')
        self.publisher_ = self.create_publisher(String, '/human_command', 10)
        self.client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
        
        # Audio Config
        self.fs = 44100  # Sample rate
        self.seconds = 5  # Duration of recording

    def record_and_transcribe(self):
        self.get_logger().info("Listening...")
        
        # Record audio
        myrecording = sd.rec(int(self.seconds * self.fs), samplerate=self.fs, channels=1)
        sd.wait()  # Wait until recording is finished
        wav.write('output.wav', self.fs, myrecording)
        
        # Transcribe via API
        audio_file = open("output.wav", "rb")
        transcription = self.client.audio.transcriptions.create(
            model="whisper-1", 
            file=audio_file
        )
        
        text = transcription.text
        self.get_logger().info(f"Heard: {text}")
        
        # Publish
        msg = String()
        msg.data = text
        self.publisher_.publish(msg)

def main(args=None):
    rclpy.init(args=args)
    node = VoiceListener()
    
    # In a real loop, you'd trigger this via a button or wake-word
    while rclpy.ok():
        input("Press Enter to speak...")
        node.record_and_transcribe()

    node.destroy_node()
    rclpy.shutdown()
```

## 1.4 Testing

1.  Export your API key: `export OPENAI_API_KEY="sk-..."`
2.  Run the node: `python3 voice_listener.py`
3.  Speak: "Walk forward three meters."
4.  Check the topic: `ros2 topic echo /human_command`.

You should see your spoken text appear on the ROS topic.
