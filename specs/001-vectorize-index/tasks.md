# P2.2 Vectorization & Indexing: OpenAI Embeddings and Dual-DB Ingestion - Tasks

**Feature Branch**: `001-vectorize-index`  
**Goal**: Create a robust, batch-processing script to generate OpenAI embeddings for all RAG chunks and simultaneously ingest vectors into Qdrant and metadata into Neon Postgres.

---

## Phase 1: Setup and Data Input

*   - [ ] T001 Create `scripts/vectorizer/` directory.
*   - [ ] T002 Create `scripts/vectorizer/main_indexer.py` file to house the main indexing script.
*   - [ ] T003 Initialize poetry in `scripts/vectorizer/`: `poetry init --no-interaction`.
*   - [ ] T004 Install Python dependencies in `scripts/vectorizer/`: `poetry add openai qdrant-client "psycopg2-binary" python-dotenv tenacity`.
*   - [ ] T005 Implement `load_rag_chunks(input_file: Path)` in `scripts/vectorizer/main_indexer.py` to read the input JSON and return a list of `RagChunk` objects.
*   - [ ] T006 Implement `chunk_batcher(chunks: List[RagChunk], batch_size: int)` generator in `scripts/vectorizer/main_indexer.py` to yield batches of chunks.

## Phase 2: User Story 1 - Generate OpenAI Embeddings for RAG Chunks [US1]

**Goal**: Generate vector embeddings for each processed RAG chunk using the OpenAI Embeddings API.
**Independent Test**: Running the script with a sample input file and verifying that for each chunk, a 1536-dimension vector is successfully generated by OpenAI API.

*   - [ ] T007 [US1] Create `scripts/vectorizer/openai_embedder.py` file.
*   - [ ] T008 [US1] Initialize OpenAI Client in `scripts/vectorizer/openai_embedder.py` using API key from environment variables.
*   - [ ] T009 [US1] Implement `get_embeddings_batch(texts: List[str]) -> List[List[float]]` function in `scripts/vectorizer/openai_embedder.py` to call the OpenAI Embeddings API.
*   - [ ] T010 [US1] Decorate `get_embeddings_batch` in `scripts/vectorizer/openai_embedder.py` with `tenacity` for exponential backoff and retry on API errors (e.g., 429 Too Many Requests).

## Phase 3: User Story 2 - Ingest Vectors into Qdrant [US2]

**Goal**: Ingest the generated vector embeddings, along with their unique `chunk_id`, into the Qdrant `book_vectors` collection.
**Independent Test**: Running the script with a sample input file and verifying that Qdrant reports the correct number of points ingested, each with the correct `chunk_id`.

*   - [ ] T011 [US2] Create `scripts/vectorizer/qdrant_ingester.py` file.
*   - [ ] T012 [US2] Implement `prepare_qdrant_points(chunks: List[RagChunk], embeddings: List[List[float]]) -> List[PointStruct]` in `scripts/vectorizer/qdrant_ingester.py` to transform chunks and embeddings into Qdrant `PointStruct` objects.
*   - [ ] T013 [US2] Implement `ingest_to_qdrant_batch(qdrant_client, points: List[PointStruct])` function in `scripts/vectorizer/qdrant_ingester.py` to perform batched `client.upsert` with `wait=True`.

## Phase 4: User Story 3 - Ingest Metadata into Neon Postgres [US3]

**Goal**: Ingest the metadata associated with each text chunk into the Neon Postgres `rag_metadata` table.
**Independent Test**: Running the script with a sample input file and verifying that Neon Postgres reports the correct number of rows inserted into the `rag_metadata` table, with correct `chunk_id` and other metadata.

*   - [ ] T014 [US3] Create `scripts/vectorizer/neon_ingester.py` file.
*   - [ ] T015 [US3] Implement `prepare_neon_data(chunks: List[RagChunk]) -> List[Tuple]` in `scripts/vectorizer/neon_ingester.py` to generate a list of tuples formatted for batched SQL insertion.
*   - [ ] T016 [US3] Implement `ingest_to_neon_batch(neon_connection, data: List[Tuple])` function in `scripts/vectorizer/neon_ingester.py` using `psycopg2.extras.execute_batch`.

## Phase 5: User Story 4 - Ensure Data Integrity [US4] & User Story 5 - Robust Error Handling and Batch Processing [US5]

**Goal**: Ensure data consistency across Qdrant and Neon, and handle API failures gracefully and efficiently using batch processing.
**Independent Test**: After a full ingestion, query Qdrant and Neon Postgres for total counts and compare with the input file's chunk count. Simulate API rate limits or connection errors.

*   - [ ] T017 [US4] [US5] Orchestrate the main indexing loop in `scripts/vectorizer/main_indexer.py` to:
    *   Read chunks in batches.
    *   Call `get_embeddings_batch` (T009/T010).
    *   Call `ingest_to_qdrant_batch` (T013).
    *   Call `ingest_to_neon_batch` (T016).
*   - [ ] T018 [US4] [US5] Implement pre- and post-ingestion count checks in `scripts/vectorizer/main_indexer.py` to verify data integrity (input JSON count vs. Qdrant count vs. Neon count).
*   - [ ] T019 [US5] Add comprehensive logging for progress, batch processing status, and errors in `scripts/vectorizer/main_indexer.py`.
*   - [ ] T020 [US5] Add retry logic and error handling around database connection failures in both Qdrant and Neon ingestion functions.

## Phase 6: Polish & Cross-Cutting Concerns

*   - [ ] T021 Implement command-line argument parsing for `input_file`, `batch_size`, etc. in `scripts/vectorizer/main_indexer.py`.
*   - [ ] T022 Create `scripts/vectorizer/.env.example` with placeholders for `OPENAI_API_KEY`, `NEON_POSTGRES_CONNECTION_STRING`, `QDRANT_HOST`, `QDRANT_API_KEY`.
*   - [ ] T023 Add comprehensive docstrings and type hints to all functions and classes in `scripts/vectorizer/`.
*   - [ ] T024 Create sample input JSON for testing.
*   - [ ] T025 Create `scripts/vectorizer/test_indexer.py` for unit/integration tests, including mocking OpenAI and database clients for API error simulation.
*   - [ ] T026 Update `pyproject.toml` in `scripts/vectorizer/` to include `pytest` for testing.

---

## Dependencies

This section outlines the completion order of user stories.

1.  **User Story 1 (Generate OpenAI Embeddings for RAG Chunks)** -> **User Story 2 (Ingest Vectors into Qdrant)** & **User Story 3 (Ingest Metadata into Neon Postgres)** -> **User Story 4 (Ensure Data Integrity)** & **User Story 5 (Robust Error Handling and Batch Processing)**

## Parallel Execution Opportunities

*   **Between Foundational Data Input (T005-T006)** and **OpenAI Embedding Generation (T007-T010)**: While embeddings depend on chunk loading, the setup of the embedder can be done in parallel with data loading.
*   **Between Qdrant Ingestion (T011-T013)** and **Neon Ingestion (T014-T016)**: Once embeddings are generated for a batch, the ingestion into Qdrant and Neon Postgres can happen concurrently for that batch, as they are independent database operations.
*   **Within Orchestration (T017-T020)**: While the main loop is sequential, logging and retry mechanisms can be developed in parallel with the core ingestion logic.

## Implementation Strategy

The implementation will follow a modular and robust approach:
1.  **Environment Setup**: Create the isolated script environment and install all necessary dependencies.
2.  **Input Processing**: Implement reliable loading and batching of RAG chunks from the JSON input.
3.  **Embedding Generation**: Develop the OpenAI embedding function with built-in retry and backoff for resilience.
4.  **Dual Ingestion**: Implement parallel or near-parallel batch ingestion logic for Qdrant (vectors) and Neon Postgres (metadata).
5.  **Orchestration & Verification**: Write the main script to orchestrate the entire process, including pre- and post-ingestion data integrity checks.
6.  **Testing & Polish**: Thoroughly test all components, particularly error handling and batching, and ensure comprehensive documentation.
