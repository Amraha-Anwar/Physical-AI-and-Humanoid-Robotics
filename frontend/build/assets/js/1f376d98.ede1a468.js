"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[121],{646:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>r,default:()=>u,frontMatter:()=>i,metadata:()=>l,toc:()=>c});var o=t(8168),a=(t(6540),t(5680));const i={sidebar_position:2,title:"Cognitive Planning with LLMs"},r="Cognitive Planning with LLMs",l={unversionedId:"module4/cognitive-planning",id:"module4/cognitive-planning",title:"Cognitive Planning with LLMs",description:"Learning Objectives",source:"@site/docs/module4/02-cognitive-planning.mdx",sourceDirName:"module4",slug:"/module4/cognitive-planning",permalink:"/docs/module4/cognitive-planning",draft:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module4/02-cognitive-planning.mdx",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,title:"Cognitive Planning with LLMs"},sidebar:"tutorialSidebar",previous:{title:"Conversational Robotics",permalink:"/docs/module4/conversational-robotics"},next:{title:"Capstone Synthesis & Sim-to-Real",permalink:"/docs/module4/capstone-synthesis"}},s={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Cognitive Planning",id:"introduction-to-cognitive-planning",level:2},{value:"1. LLMs for Task Decomposition",id:"1-llms-for-task-decomposition",level:2},{value:"OpenAI API Syntax for Structured Output",id:"openai-api-syntax-for-structured-output",level:3},{value:"Explanation of Pseudo-code:",id:"explanation-of-pseudo-code",level:3},{value:"3. Integrating into ROS 2 Ecosystem",id:"3-integrating-into-ros-2-ecosystem",level:2},{value:"Conclusion",id:"conclusion",level:2}],p={toc:c},g="wrapper";function u({components:e,...n}){return(0,a.yg)(g,(0,o.A)({},p,n,{components:e,mdxType:"MDXLayout"}),(0,a.yg)("h1",{id:"cognitive-planning-with-llms"},"Cognitive Planning with LLMs"),(0,a.yg)("h2",{id:"learning-objectives"},"Learning Objectives"),(0,a.yg)("p",null,"Upon completing this chapter, you will be able to:"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Understand the role of Large Language Models (LLMs) in abstract cognitive planning for robots."),(0,a.yg)("li",{parentName:"ul"},"Learn how to use an LLM (with OpenAI API syntax) to translate natural language goals into sequences of robotic actions."),(0,a.yg)("li",{parentName:"ul"},"Implement the LLM-to-ROS translation logic, ensuring structured output for reliable execution."),(0,a.yg)("li",{parentName:"ul"},"Integrate LLM-generated plans into a ROS 2 system.")),(0,a.yg)("h2",{id:"introduction-to-cognitive-planning"},"Introduction to Cognitive Planning"),(0,a.yg)("p",null,"Traditional robot planning often relies on predefined state machines, rule-based systems, or complex symbolic AI techniques. While effective for well-defined problems, these approaches struggle with ambiguity, novel situations, and the open-ended nature of human commands. ",(0,a.yg)("strong",{parentName:"p"},"Cognitive planning"),", particularly with the advent of Large Language Models (LLMs), offers a powerful paradigm shift."),(0,a.yg)("p",null,'LLMs can bridge the gap between high-level, abstract natural language goals (e.g., "Make me coffee," "Clean the room") and the low-level, concrete actions a robot can perform. They can:'),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Interpret ambiguity"),": Understand nuanced human instructions."),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Reason about context"),": Infer implicit steps based on common sense knowledge."),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Generate sequences"),": Decompose a complex goal into a series of simpler, executable steps."),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Handle exceptions"),": Potentially suggest recovery strategies for failures.")),(0,a.yg)("h2",{id:"1-llms-for-task-decomposition"},"1. LLMs for Task Decomposition"),(0,a.yg)("p",null,"The core idea is to prompt an LLM with a high-level goal and a list of available robot actions, asking it to return a structured sequence of these actions to achieve the goal."),(0,a.yg)("h3",{id:"openai-api-syntax-for-structured-output"},"OpenAI API Syntax for Structured Output"),(0,a.yg)("p",null,"To ensure reliable execution, the LLM's output must be structured and parseable. Modern LLMs support JSON output, which is ideal for this. We'll use a ",(0,a.yg)("inlineCode",{parentName:"p"},"system")," prompt to define the robot's capabilities and the desired output format, and a ",(0,a.yg)("inlineCode",{parentName:"p"},"user")," prompt for the specific task."),(0,a.yg)("p",null,"Let's assume our humanoid robot has the following basic actions:"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"walk(direction)"),": ",(0,a.yg)("inlineCode",{parentName:"li"},"direction"),' can be "forward", "backward", "left", "right".'),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"turn(direction)"),": ",(0,a.yg)("inlineCode",{parentName:"li"},"direction"),' can be "left", "right".'),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"look(object)"),": ",(0,a.yg)("inlineCode",{parentName:"li"},"object"),' can be "table", "door", "person".'),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"pick_up(object)"),": ",(0,a.yg)("inlineCode",{parentName:"li"},"object"),' can be "cup", "ball".'),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"place_down(object)"),": ",(0,a.yg)("inlineCode",{parentName:"li"},"object"),' can be "cup", "ball".')),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"LLM Prompt Design:")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},'system_prompt = """\nYou are a robotic task planner. Your goal is to break down high-level natural language instructions into a sequence of low-level robot actions.\nThe robot can perform the following actions:\n- walk(direction: str) - Moves the robot. Directions: "forward", "backward", "left", "right".\n- turn(direction: str) - Rotates the robot. Directions: "left", "right".\n- look(object: str) - Orients the robot\'s head/camera towards an object. Objects: "table", "door", "person", "cup", "ball".\n- pick_up(object: str) - Picks up a specified object. Objects: "cup", "ball".\n- place_down(object: str) - Places down a specified object. Objects: "cup", "ball".\n\nReturn the plan as a JSON array of action objects. Each action object must have a \'name\' (string) and \'args\' (object mapping argument names to values).\n\nExample:\nUser: "Go get the cup from the table."\nOutput:\n```json\n[\n  {"name": "walk", "args": {"direction": "forward"}},\n  {"name": "look", "args": {"object": "table"}},\n  {"name": "walk", "args": {"direction": "forward"}},\n  {"name": "pick_up", "args": {"object": "cup"}},\n  {"name": "walk", "args": {"direction": "backward"}},\n  {"name": "place_down", "args": {"object": "cup"}}\n]\n')),(0,a.yg)("p",null,'If an instruction is unclear or an object/action is not supported, return an empty array or indicate impossibility.\n"""'),(0,a.yg)("p",null,'user_instruction = "Please go to the door, then pick up the red ball and bring it here." # Example user input'),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre"},'\n## 2. LLM-to-ROS Translation Logic (Pseudo-code)\n\nThis logic involves:\n1.  Sending the prompt to the LLM (e.g., OpenAI API).\n2.  Parsing the JSON response.\n3.  Translating each LLM-generated action into a corresponding ROS 2 message or action goal.\n4.  Executing the sequence of ROS 2 actions.\n\n**Pseudo-code Example (Python with `rclpy` and conceptual OpenAI API):**\n\n```python\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\n# from custom_action_msgs.action import PickUp, PlaceDown # Conceptual custom action types\nimport json\nimport requests # For conceptual OpenAI API call\n\n# --- ROS 2 Message Definitions for Actions (conceptual) ---\n# For simplicity, we\'ll use Twist for walk/turn, and String for look.\n# For pick_up/place_down, you would typically use ROS 2 Actions.\n\n# Assuming OpenAI API key is set as an environment variable\nOPENAI_API_KEY = "YOUR_OPENAI_API_KEY" # In a real app, use env vars or secrets\n\nclass CognitivePlanner(Node):\n    def __init__(self):\n        super().__init__(\'cognitive_planner\')\n        self.declare_parameter(\'openai_api_endpoint\', \'https://api.openai.com/v1/chat/completions\')\n        self.openai_api_endpoint = self.get_parameter(\'openai_api_endpoint\').get_parameter_value().string_value\n\n        self.get_logger().info("Cognitive Planner node started.")\n        self.get_logger().info(f"OpenAI API Endpoint: {self.openai_api_endpoint}")\n\n        # Subscribe to high-level natural language goals (from voice command, GUI, etc.)\n        self.goal_sub = self.create_subscription(String, \'human_goals\', self.goal_callback, 10)\n\n        # Publishers for low-level robot actions\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n        self.look_at_pub = self.create_publisher(String, \'look_at_object\', 10) # Custom topic for look action\n        # self.pick_up_client = ActionClient(self, PickUp, \'pick_up_action\') # Conceptual action client\n        # self.place_down_client = ActionClient(self, PlaceDown, \'place_down_action\') # Conceptual action client\n\n        self.robot_actions = {\n            "walk": self.execute_walk_action,\n            "turn": self.execute_turn_action,\n            "look": self.execute_look_action,\n            "pick_up": self.execute_pick_up_action, # Placeholder\n            "place_down": self.execute_place_down_action, # Placeholder\n        }\n\n        self.system_prompt = """\n        You are a robotic task planner. Your goal is to break down high-level natural language instructions into a sequence of low-level robot actions.\n        The robot can perform the following actions:\n        - walk(direction: str) - Moves the robot. Directions: "forward", "backward", "left", "right".\n        - turn(direction: str) - Rotates the robot. Directions: "left", "right".\n        - look(object: str) - Orients the robot\'s head/camera towards an object. Objects: "table", "door", "person", "cup", "ball".\n        - pick_up(object: str) - Picks up a specified object. Objects: "cup", "ball".\n        - place_down(object: str) - Places down a specified object. Objects: "cup", "ball".\n\n        Return the plan as a JSON array of action objects. Each action object must have a \'name\' (string) and \'args\' (object mapping argument names to values).\n\n        Example:\n        User: "Go get the cup from the table."\n        Output:\n        ```json\n        [\n          {"name": "walk", "args": {"direction": "forward"}},\n          {"name": "look", "args": {"object": "table"}},\n          {"name": "walk", "args": {"direction": "forward"}},\n          {"name": "pick_up", "args": {"object": "cup"}},\n          {"name": "walk", "args": {"direction": "backward"}},\n          {"name": "place_down", "args": {"object": "cup"}}\n        ]\n        ```\n        If an instruction is unclear or an object/action is not supported, return an empty array or indicate impossibility.\n        """\n\n    def call_llm_api(self, user_instruction):\n        headers = {\n            "Content-Type": "application/json",\n            "Authorization": f"Bearer {OPENAI_API_KEY}"\n        }\n        payload = {\n            "model": "gpt-4-turbo", # Or another suitable model\n            "messages": [\n                {"role": "system", "content": self.system_prompt},\n                {"role": "user", "content": user_instruction}\n            ],\n            "response_format": {"type": "json_object"} # Crucial for structured output\n        }\n        \n        try:\n            response = requests.post(self.openai_api_endpoint, headers=headers, json=payload)\n            response.raise_for_status() # Raise an exception for HTTP errors\n            response_json = response.json()\n            \n            # Extract content from the response\n            content = response_json[\'choices\'][0][\'message\'][\'content\']\n            return json.loads(content) # Parse the JSON string from the LLM\n        except requests.exceptions.RequestException as e:\n            self.get_logger().error(f"Error calling OpenAI API: {e}")\n            return []\n        except json.JSONDecodeError as e:\n            self.get_logger().error(f"Error decoding JSON from LLM: {e}. Raw content: {content}")\n            return []\n        except KeyError as e:\n            self.get_logger().error(f"Unexpected LLM response structure: {e}. Response: {response_json}")\n            return []\n\n    def goal_callback(self, msg):\n        human_goal = msg.data\n        self.get_logger().info(f"Received high-level goal: \'{human_goal}\'")\n\n        # 1. Get plan from LLM\n        action_plan = self.call_llm_api(human_goal)\n        if not action_plan:\n            self.get_logger().warn("LLM failed to generate a valid plan or returned an empty plan.")\n            return\n\n        self.get_logger().info(f"LLM-generated plan: {action_plan}")\n\n        # 2. Execute plan\n        for action in action_plan:\n            action_name = action.get("name")\n            action_args = action.get("args", {})\n\n            if action_name in self.robot_actions:\n                self.get_logger().info(f"Executing action: {action_name} with args: {action_args}")\n                self.robot_actions[action_name](**action_args)\n                # In a real system, you\'d wait for action completion before proceeding\n                rclpy.spin_once(self, timeout_sec=1.0) # Allow ROS to process\n            else:\n                self.get_logger().error(f"Unknown action \'{action_name}\' in plan. Skipping.")\n                return # Stop execution if an unknown action is encountered\n        \n        self.get_logger().info("Plan execution completed.")\n\n    # --- Low-level ROS 2 Action Execution Methods ---\n    def execute_walk_action(self, direction):\n        twist_msg = Twist()\n        if direction == "forward":\n            twist_msg.linear.x = 0.2\n        elif direction == "backward":\n            twist_msg.linear.x = -0.1\n        elif direction == "left":\n            twist_msg.linear.y = 0.1\n        elif direction == "right":\n            twist_msg.linear.y = -0.1\n        else:\n            self.get_logger().warn(f"Invalid walk direction: {direction}")\n            return\n        self.cmd_vel_pub.publish(twist_msg)\n        # Simulate action duration\n        self.get_logger().info(f"Robot walking {direction}...")\n        # In a real robot, this would be a feedback loop or a blocking action client call\n\n    def execute_turn_action(self, direction):\n        twist_msg = Twist()\n        if direction == "left":\n            twist_msg.angular.z = 0.5\n        elif direction == "right":\n            twist_msg.angular.z = -0.5\n        else:\n            self.get_logger().warn(f"Invalid turn direction: {direction}")\n            return\n        self.cmd_vel_pub.publish(twist_msg)\n        self.get_logger().info(f"Robot turning {direction}...")\n\n    def execute_look_action(self, object):\n        look_msg = String()\n        look_msg.data = object\n        self.look_at_pub.publish(look_msg)\n        self.get_logger().info(f"Robot looking at {object}...")\n\n    def execute_pick_up_action(self, object):\n        self.get_logger().info(f"Robot attempting to pick up {object}...")\n        # In a real system, this would involve calling a pick_up_action client goal\n        # For example: self.pick_up_client.wait_for_server(); goal = PickUp.Goal(object_name=object); future = self.pick_up_client.send_goal_async(goal)\n\n    def execute_place_down_action(self, object):\n        self.get_logger().info(f"Robot attempting to place down {object}...")\n        # In a real system, this would involve calling a place_down_action client goal\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    cognitive_planner_node = CognitivePlanner()\n    \n    # Simulate a human goal\n    # In a real scenario, this would come from the voice command node or a GUI\n    human_goal_publisher = cognitive_planner_node.create_publisher(String, \'human_goals\', 10)\n    \n    # Give some time for publishers/subscribers to connect\n    import time\n    time.sleep(2) \n    \n    simulated_goal = String()\n    simulated_goal.data = "Go to the door, then pick up the cup from the table and place it down."\n    cognitive_planner_node.get_logger().info(f"Simulating human goal: {simulated_goal.data}")\n    human_goal_publisher.publish(simulated_goal)\n\n    rclpy.spin(cognitive_planner_node)\n\n    cognitive_planner_node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n')),(0,a.yg)("p",null,(0,a.yg)("em",{parentName:"p"},"Pseudo-code: ",(0,a.yg)("inlineCode",{parentName:"em"},"cognitive_planner.py"))),(0,a.yg)("h3",{id:"explanation-of-pseudo-code"},"Explanation of Pseudo-code:"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},(0,a.yg)("inlineCode",{parentName:"strong"},"CognitivePlanner")," Node"),": Subscribes to ",(0,a.yg)("inlineCode",{parentName:"li"},"human_goals")," (natural language input) and publishes to low-level action topics (",(0,a.yg)("inlineCode",{parentName:"li"},"cmd_vel"),", ",(0,a.yg)("inlineCode",{parentName:"li"},"look_at_object"),"). It also conceptually uses ",(0,a.yg)("inlineCode",{parentName:"li"},"ActionClient")," for ",(0,a.yg)("inlineCode",{parentName:"li"},"pick_up"),"/",(0,a.yg)("inlineCode",{parentName:"li"},"place_down")," actions."),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},(0,a.yg)("inlineCode",{parentName:"strong"},"call_llm_api")),": A helper function to interact with the OpenAI API. It constructs the prompt using the ",(0,a.yg)("inlineCode",{parentName:"li"},"system_prompt")," (defining robot capabilities and output format) and the ",(0,a.yg)("inlineCode",{parentName:"li"},"user_instruction"),". Crucially, it sets ",(0,a.yg)("inlineCode",{parentName:"li"},'response_format={"type": "json_object"}')," to enforce structured JSON output."),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},(0,a.yg)("inlineCode",{parentName:"strong"},"goal_callback")),": When a new high-level goal is received, it calls the LLM, parses the JSON plan, and then iterates through the actions, calling the appropriate ROS 2 execution method."),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Low-level Action Methods"),": ",(0,a.yg)("inlineCode",{parentName:"li"},"execute_walk_action"),", ",(0,a.yg)("inlineCode",{parentName:"li"},"execute_turn_action"),", ",(0,a.yg)("inlineCode",{parentName:"li"},"execute_look_action")," translate the LLM's structured actions into ROS 2 ",(0,a.yg)("inlineCode",{parentName:"li"},"Twist")," messages or custom messages. For more complex actions like ",(0,a.yg)("inlineCode",{parentName:"li"},"pick_up")," or ",(0,a.yg)("inlineCode",{parentName:"li"},"place_down"),", these would typically involve ROS 2 Action Servers/Clients for robust execution with feedback and preemption.")),(0,a.yg)("h2",{id:"3-integrating-into-ros-2-ecosystem"},"3. Integrating into ROS 2 Ecosystem"),(0,a.yg)("p",null,"To make ",(0,a.yg)("inlineCode",{parentName:"p"},"cognitive_planner.py")," executable within your ROS 2 environment, ensure it's placed in a package (e.g., ",(0,a.yg)("inlineCode",{parentName:"p"},"humanoid_vla_pkg"),") and its entry point is defined in the ",(0,a.yg)("inlineCode",{parentName:"p"},"setup.py")," of that package:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-python"},"# ... inside setup() function ...\n    entry_points={\n        'console_scripts': [\n            'cognitive_planner = humanoid_vla_pkg.cognitive_planner:main',\n        ],\n    },\n")),(0,a.yg)("p",null,"After building your package (",(0,a.yg)("inlineCode",{parentName:"p"},"colcon build --packages-select humanoid_vla_pkg"),") and sourcing your workspace, you can run the node:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-bash"},"ros2 run humanoid_vla_pkg cognitive_planner\n")),(0,a.yg)("p",null,"This node would then listen for goals on the ",(0,a.yg)("inlineCode",{parentName:"p"},"/human_goals")," topic. In a complete system, the ",(0,a.yg)("inlineCode",{parentName:"p"},"VoiceCommandTranscriber")," node from the previous chapter would publish to this topic."),(0,a.yg)("h2",{id:"conclusion"},"Conclusion"),(0,a.yg)("p",null,"By leveraging LLMs for cognitive planning, robots can move beyond rigid, pre-programmed behaviors to truly understand and execute complex, high-level natural language commands. The ability to generate structured action plans from LLMs and translate them into executable ROS 2 messages is a significant step towards more autonomous and human-friendly robotic systems."),(0,a.yg)("hr",null),(0,a.yg)("p",null,(0,a.yg)("strong",{parentName:"p"},"Next Steps"),": With the cognitive brain in place, the final chapter will synthesize all learned concepts into a Capstone project, focusing on the crucial aspects of testing, integration, and the Sim-to-Real transfer of these complex humanoid capabilities."))}u.isMDXComponent=!0},5680:(e,n,t)=>{t.d(n,{xA:()=>p,yg:()=>m});var o=t(6540);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,o)}return t}function r(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach(function(n){a(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function l(e,n){if(null==e)return{};var t,o,a=function(e,n){if(null==e)return{};var t,o,a={},i=Object.keys(e);for(o=0;o<i.length;o++)t=i[o],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)t=i[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=o.createContext({}),c=function(e){var n=o.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):r(r({},n),e)),t},p=function(e){var n=c(e.components);return o.createElement(s.Provider,{value:n},e.children)},g="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return o.createElement(o.Fragment,{},n)}},d=o.forwardRef(function(e,n){var t=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),g=c(t),d=a,m=g["".concat(s,".").concat(d)]||g[d]||u[d]||i;return t?o.createElement(m,r(r({ref:n},p),{},{components:t})):o.createElement(m,r({ref:n},p))});function m(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var i=t.length,r=new Array(i);r[0]=d;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[g]="string"==typeof e?e:a,r[1]=l;for(var c=2;c<i;c++)r[c]=t[c];return o.createElement.apply(null,r)}return o.createElement.apply(null,t)}d.displayName="MDXCreateElement"}}]);