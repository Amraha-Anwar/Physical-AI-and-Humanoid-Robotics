# P2.3 Retrieval & Agent Logic: Core RAG FastAPI Endpoint - Data Model

This document outlines the data models and flow within the FastAPI RAG endpoint. It builds upon the `RagChunk` model (from P1.3) and integrates data from Qdrant and Neon Postgres to generate contextual answers using OpenAI's LLM.

## 1. Input Data Model

**Entity**: `RAGQueryRequest` (Pydantic Model)

**Description**: Represents the input payload for the `/api/rag/query` endpoint.

**Attributes**:
*   `query` (str): The user's natural language question.
*   `context_snippet` (str, Optional): An optional text snippet provided by the user (e.g., via highlight-to-query).

**Pydantic Model (Conceptual)**:

```python
from pydantic import BaseModel
from typing import Optional

class RAGQueryRequest(BaseModel):
    query: str
    context_snippet: Optional[str] = None
```

## 2. Intermediate Data Models

### 2.1 Query Embedding

**Entity**: `Query Embedding`

**Description**: The vector representation of the `query` (or `context_snippet` if it's used as the primary search query) generated by the OpenAI Embeddings API.

**Key Characteristics**:
*   Dimension: 1536 (matching `book_vectors` in Qdrant).
*   Type: List of floats.

### 2.2 Retrieved Chunks

**Entity**: `Retrieved Chunks` (List of `RagChunk` objects)

**Description**: A collection of `RagChunk` objects (including their full text and metadata) identified as relevant from Qdrant and retrieved from Neon Postgres.

**Key Characteristics**:
*   Each item is a `RagChunk` object (as defined in `backend/models.py`).
*   The number of chunks (`N`) is configurable (e.g., 5).

## 3. Core Processing Models

### 3.1 Prompt Template

**Entity**: `Prompt Template`

**Description**: A structured string used to combine the `User Query` and `Retrieved Chunks` (or `Context Snippet`) into an effective input for the OpenAI LLM. It includes instructions for the LLM.

**Key Components**:
*   System Instruction: Ensures grounding ("answer *only* from context").
*   Context Placeholder: Where `Retrieved Chunks` text is injected.
*   User Query Placeholder.

### 3.2 OpenAI LLM Request / Response

**Entity**: `OpenAI Chat API Request` / `Response`

**Description**: The message structure sent to and received from the OpenAI Chat Completion API.

**Key Attributes (Request)**:
*   `model` (str): e.g., `gpt-4-turbo`
*   `messages` (List[Dict]): Conversation history (system message, user query, context).
*   `temperature` (float): e.g., 0.0-0.3 for factual consistency.

**Key Attributes (Response)**:
*   `content` (str): The generated answer from the LLM.

## 4. Output Data Model

**Entity**: `RAGQueryResponse` (Pydantic Model)

**Description**: Represents the output payload returned by the `/api/rag/query` endpoint.

**Attributes**:
*   `answer` (str): The contextual answer generated by the LLM.
*   `sources` (List[Dict], Optional): Metadata about the `Retrieved Chunks` (e.g., `chunk_id`, `chapter_title`) to attribute the answer.

**Pydantic Model (Conceptual)**:

```python
from pydantic import BaseModel
from typing import List, Dict, Optional

class RAGQueryResponse(BaseModel):
    answer: str
    sources: Optional[List[Dict]] = None # e.g., [{"chunk_id": "...", "chapter_title": "..."}]
```

## 5. Data Flow within Endpoint

1.  **Request Parsing**: The `/api/rag/query` endpoint receives a `RAGQueryRequest`.
2.  **Context Routing**:
    *   If `context_snippet` is present:
        *   `context_snippet` is used directly as context.
    *   If `context_snippet` is absent:
        *   `User Query` is embedded via OpenAI Embeddings API to produce `Query Embedding`.
        *   `Query Embedding` is used to search `Qdrant` for `Retrieved Chunks` (`chunk_id`s).
        *   `chunk_id`s are used to fetch full `RagChunk` data from `Neon Postgres`.
3.  **Prompt Construction**: `Retrieved Chunks` (or `context_snippet`) and `User Query` are combined into `Prompt Template`.
4.  **LLM Generation**: `Prompt Template` is sent to `OpenAI LLM` to get `Contextual Answer`.
5.  **Response Formatting**: `Contextual Answer` is formatted into `RAGQueryResponse` and returned.
