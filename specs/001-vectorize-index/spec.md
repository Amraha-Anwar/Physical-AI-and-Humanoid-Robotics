# Feature Specification: P2.2 Vectorization & Indexing: OpenAI Embeddings and Dual-DB Ingestion

**Feature Branch**: `001-vectorize-index`  
**Created**: 2025-12-07  
**Status**: Draft  
**Input**: User description: "P2.2 Vectorization & Indexing: OpenAI Embeddings and Dual-DB Ingestion Target module: P2.2 Vectorization & Indexing Focus: Create a dedicated indexing script to perform the data ingestion workflow: read the processed RAG chunks, generate vectors using the OpenAI API, and store the vectors and corresponding metadata into Qdrant and Neon Postgres. Success criteria: - **Vectorization**: The script successfully calls the **OpenAI Embeddings API** for each text chunk (T3.3 from P2.1) and receives the vector representation (1536 dimensions). - **Qdrant Ingestion**: The vector and its unique `chunk_id` (T3.4 from P2.1) are successfully uploaded as a PointStruct to the `book_vectors` collection in Qdrant. - **Neon Metadata Insertion**: The chunk's metadata (`chunk_id`, `text`, `chapter_title`, etc.) is successfully inserted as a row into the `rag_metadata` table in Neon Postgres. - **Data Integrity**: The final count of points in Qdrant must exactly match the number of rows inserted into the Neon Postgres table and the number of chunks processed from the input JSON file. - **Error Handling**: The script must include robust error handling for API failures (rate limits, connection errors) and utilize batch processing for efficiency. Constraints: - **Input Source**: Must use the structured JSON file output generated by the P2.1 script. - **API Tooling**: Must use the official **OpenAI Python SDK** for embedding generation and the previously defined `qdrant_client.py` and `neon_client.py` modules from P1.3. - **Batch Processing**: Vectorization and ingestion operations must utilize **batching** (e.g., 50-100 chunks at a time) to optimize performance and reduce API call overhead. - **One-Time Run**: This script is designed for a single, full index run. Not building: - The API endpoint for real-time query embedding or retrieval (P2.3). - The definition of database schemas (completed in P1.3). - Any content cleaning or chunking logic (completed in P2.1)."

## User Scenarios & Testing

### User Story 1 - Generate OpenAI Embeddings for RAG Chunks (Priority: P1)
As a data engineer, I want to generate vector embeddings for each processed RAG chunk using the OpenAI Embeddings API, so that the textual content can be represented in a numerical format suitable for vector similarity search.
**Why this priority**: This is a core step for enabling semantic search and Retrieval-Augmented Generation (RAG) functionality.
**Independent Test**: Running the script with a sample input file and verifying that for each chunk, a 1536-dimension vector is successfully generated by the OpenAI API.
**Acceptance Scenarios**:
1.  **Given** a JSON file containing processed `RagChunk` objects (output from P2.1) and a configured OpenAI API key, **When** the indexing script processes these chunks, **Then** for each text chunk, a corresponding vector embedding of 1536 dimensions is successfully obtained from the OpenAI Embeddings API.

### User Story 2 - Ingest Vectors into Qdrant (Priority: P1)
As a data engineer, I want to ingest the generated vector embeddings, along with their unique `chunk_id`, into the Qdrant `book_vectors` collection, so that they are indexed and available for fast similarity search.
**Why this priority**: Essential for vector storage and retrieval components of the RAG system.
**Independent Test**: Running the script with a sample input file and verifying that Qdrant reports the correct number of points ingested, each with the correct `chunk_id`.
**Acceptance Scenarios**:
1.  **Given** generated vector embeddings and `chunk_id` for each text chunk, **When** the indexing script performs ingestion, **Then** each vector is successfully uploaded as a `PointStruct` to the Qdrant `book_vectors` collection, using `chunk_id` as the point's ID.

### User Story 3 - Ingest Metadata into Neon Postgres (Priority: P1)
As a data engineer, I want to ingest the metadata associated with each text chunk into the Neon Postgres `rag_metadata` table, so that I can easily query and retrieve contextual information related to the stored vectors.
**Why this priority**: Crucial for storing and managing human-readable metadata linked to vectors, enabling context retrieval after a vector search.
**Independent Test**: Running the script with a sample input file and verifying that Neon Postgres reports the correct number of rows inserted into the `rag_metadata` table, with correct `chunk_id` and other metadata.
**Acceptance Scenarios**:
1.  **Given** `RagChunk` objects containing metadata, **When** the indexing script performs ingestion, **Then** each chunk's metadata (including `chunk_id`, `text_snippet_preview`, `chapter_title`, etc.) is successfully inserted as a row into the Neon Postgres `rag_metadata` table.

### User Story 4 - Ensure Data Integrity (Priority: P2)
As a data engineer, I want to ensure that the number of ingested vectors in Qdrant perfectly matches the number of metadata rows in Neon Postgres and the number of processed chunks from the input file, so that data consistency is maintained across the dual databases.
**Why this priority**: Critical for reliable RAG functionality; inconsistent data could lead to missing or incorrect responses.
**Independent Test**: After running a full ingestion, query Qdrant and Neon Postgres for total counts and compare with the input file's chunk count.
**Acceptance Scenarios**:
1.  **Given** the indexing script has completed processing an input JSON file, **When** I query the total count of points in Qdrant's `book_vectors` collection, **Then** this count exactly matches the total number of rows in Neon Postgres's `rag_metadata` table, and both counts match the initial number of `RagChunk` objects from the input JSON file.

### User Story 5 - Robust Error Handling and Batch Processing (Priority: P2)
As a data engineer, I want the indexing script to handle API failures (e.g., rate limits, connection errors) gracefully and utilize batch processing for both vectorization and ingestion, so that the indexing process is resilient and efficient.
**Why this priority**: Improves reliability and performance of the ingestion pipeline, especially with large datasets, and reduces operational costs.
**Independent Test**: Simulate OpenAI API rate limits or Qdrant/Neon connection errors and observe script's behavior (retries, logging, graceful exit). Verify that API calls are batched.
**Acceptance Scenarios**:
1.  **Given** a large input JSON file, **When** the script performs vectorization and ingestion, **Then** OpenAI API calls and database writes to Qdrant and Neon are executed in batches (e.g., 50-100 chunks per batch), optimizing performance and reducing API overhead.
2.  **Given** a transient OpenAI API error or database connection issue, **When** the script encounters such an error, **Then** it attempts retries with exponential backoff and logs informative error messages before eventually failing gracefully if errors persist.

---

### Edge Cases

-   What happens if the input JSON file is malformed or empty? (The script should report an error and exit gracefully.)
-   How does the script handle `RagChunk` objects with missing `text_snippet_preview`? (The script should skip such chunks, log a warning, and continue processing.)
-   What if the OpenAI API key is invalid or missing? (The script should report an error and exit before making API calls.)
-   What if the Qdrant `book_vectors` collection or Neon `rag_metadata` table already contain data with conflicting `chunk_id`s? (The script should be configured to either skip existing entries, update them, or report an error based on a clear strategy, but the current context implies a one-time full index run so potential conflicts may be handled by initial database cleanup.)
-   What if an individual chunk's text is too long for the OpenAI embedding API's context window? (The script should log a warning, potentially truncate the text, or skip the chunk if truncation is not feasible, to avoid API errors.)
-   How does the script manage OpenAI API rate limits and connection timeouts effectively? (The script should implement retry mechanisms with exponential backoff and configurable timeouts for API calls and database operations.)

## Requirements

### Functional Requirements

-   **FR-001**: The script MUST read a structured JSON file containing `RagChunk` objects, which is the output from the P2.1 content pre-processing script.
-   **FR-002**: For each `RagChunk`'s `text_snippet_preview`, the script MUST call the **OpenAI Embeddings API** to generate a vector embedding (1536 dimensions, as per current OpenAI models).
-   **FR-003**: The script MUST utilize the previously defined `qdrant_client.py` module (from P1.3) to establish a connection to Qdrant.
-   **FR-004**: The script MUST upload the generated vector embedding and its corresponding `chunk_id` as a `PointStruct` to the `book_vectors` collection in Qdrant.
-   **FR-005**: The script MUST utilize the previously defined `neon_client.py` module (from P1.3) to establish a connection to Neon Postgres.
-   **FR-006**: The script MUST insert the `RagChunk`'s metadata (all fields except the vector) as a new row into the `rag_metadata` table in Neon Postgres.
-   **FR-007**: The script MUST perform both OpenAI API calls and database ingestion operations (Qdrant and Neon) using **batch processing** (configurable batch size, e.g., 50-100 chunks).
-   **FR-008**: The script MUST include robust error handling for OpenAI API failures (e.g., rate limits, invalid API key, network errors) and database connection/write failures (Qdrant, Neon), including retry mechanisms.
-   **FR-009**: The script MUST log progress, batch processing status, and any encountered errors effectively to the console or a log file.
-   **FR-010**: The script MUST ensure data integrity such that for every `RagChunk` object successfully processed, a corresponding entry is created in both Qdrant and Neon Postgres.
-   **FR-011**: The script MUST be designed for a single, full index run (one-time data ingestion process for a complete dataset).
-   **FR-012**: The script MUST use the official **OpenAI Python SDK** for embedding generation.

### Key Entities

-   **Input JSON File**: The structured file containing `RagChunk` objects, produced by the P2.1 script.
-   **RagChunk Object**: A Pydantic model instance from `backend/models.py`, encapsulating a text chunk with its metadata.
-   **OpenAI Embeddings API**: External service used to transform text into vector representations.
-   **Vector Embedding**: A 1536-dimensional numerical representation of a `RagChunk`'s text content.
-   **Qdrant PointStruct**: The data structure used to store a vector and its associated payload (metadata) in Qdrant.
-   **Neon Postgres Row**: A record in the `rag_metadata` table storing the textual metadata of a `RagChunk`, linked by `chunk_id`.

## Success Criteria

### Measurable Outcomes

-   **SC-001**: The script successfully processes a JSON input file containing 1000 `RagChunk` objects, generating embeddings and ingesting data into both Qdrant and Neon Postgres, within 5 minutes.
-   **SC-002**: After a full ingestion run, the number of points in Qdrant's `book_vectors` collection exactly matches the number of rows in Neon Postgres's `rag_metadata` table, and both counts precisely match the initial number of `RagChunk` objects from the input JSON file.
-   **SC-003**: For a simulated OpenAI API rate limit event, the script successfully retries API calls (e.g., using exponential backoff) and completes the entire processing without unhandled exceptions, demonstrating robust error handling.
-   **SC-004**: The average batch size used for OpenAI API calls and database writes is configurable (e.g., via command-line argument or environment variable) and adheres to a specified range (e.g., 50-100 chunks per batch).
-   **SC-005**: All ingested vectors in Qdrant have exactly 1536 dimensions, as verified by querying Qdrant for a sample point or checking collection configuration.
-   **SC-006**: The script provides clear and informative console output or logs indicating processing progress, current batch number, and any encountered errors or warnings.